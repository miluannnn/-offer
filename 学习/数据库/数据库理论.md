# 数据库理论

[TOC]

## 一、事务

### 什么是事务

事务是逻辑上的一组操作，要么都执行，要么都不执行。
事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。

### 事务的四大特性(ACID)

1. **原子性（Atomicity）**： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；
2. **一致性（Consistency）**： 执行事务后，数据库从一个正确的状态变化到另一个正确的状态；
3. **隔离性（Isolation）**： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；
4. **持久性（Durability）**： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。

## 二、并发一致性问题

在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对同一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。

- **脏读**（Dirty read）: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。
- **丢失修改**（Lost to modify）: 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。
- **不可重复读**（Unrepeatable-read）: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。
- **幻读**（Phantom read）: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。

**不可重复读和幻读区别：**
不可重复读的重点是修改比如多次读取一条记录发现其中某些列的值被修改，幻读的重点在于新增或者删除比如多次读取一条记录发现记录增多或减少了。

### 事务隔离级别有哪些?MySQL的默认隔离级别是?

**SQL 标准定义了四个隔离级别：**

- **READ-UNCOMMITTED(读取未提交)**： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。
- **READ-COMMITTED(读取已提交)**： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。
- **REPEATABLE-READ(可重复读)**： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。
- **SERIALIZABLE(可串行化)**： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。

## 三、索引

### 什么是索引 

索引是一种用于快速查询和检索数据的数据结构。常见的索引结构有: B树， B+树和Hash。
索引的作用就相当于目录的作用。打个比方: 我们在查字典的时候，如果没有目录，那我们就只能一页一页的去找我们需要查的那个字，速度很慢。如果有目录了，我们只需要先去目录里查找字的位置，然后直接翻到那一页就行了。

### 为什么要用索引?索引的优缺点分析

**索引的优点**
可以大大加快 数据的检索速度（大大减少的检索的数据量）, 这也是创建索引的最主要的原因。毕竟大部分系统的读请求总是大于写请求的。 另外，通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。

**索引的缺点**
创建索引和维护索引需要耗费许多时间：当对表中的数据进行增删改的时候，如果数据有索引，那么索引也需要动态的修改，会降低SQL执行效率。
占用物理存储空间 ：索引需要使用物理文件存储，也会耗费一定空间。

### 索引设计的原则

1）适合索引的列是出现在where子句中的列，或者连接子句中指定的列；
2）基数较小的类，索引效果较差，没有必要在此列建立索引；
3）使用短索引，如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间；
4）不要过度索引。索引需要额外的磁盘空间，并降低写操作的性能。在修改表内容的时候，索引会进行更新甚至重构，索引列越多，这个时间就会越长。所以只保持需要的索引有利于查询即可。



### B树和B+树的区别

- B 树的所有节点既存放 键(key) 也存放 数据(data);而 B+树只有叶子节点存放 key 和 data，其他内节点只存放 key。
- B 树的叶子节点都是独立的;B+树的叶子节点有一条引用链指向与它相邻的叶子节点。
- B 树的检索的过程相当于对范围内的每个节点的关键字做二分查找，可能还没有到达叶子节点，检索就结束了。而 B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程，叶子节点的顺序检索很明显。

![B+树](https://snailclimb.gitee.io/javaguide/media/pictures/database/B+%E6%A0%91.png)

### Hash 索引和 B+树索引优劣分析

**Hash 索引定位快**

Hash 索引指的就是 Hash 表，最大的优点就是能够在很短的时间内，根据 Hash 函数定位到数据所在的位置，这是 B+树所不能比的。

**Hash 冲突问题**

知道 HashMap 或 HashTable 的同学，相信都知道它们最大的缺点就是 Hash 冲突了。不过对于数据库来说这还不算最大的缺点。

**Hash 索引不支持顺序和范围查询(Hash 索引不支持顺序和范围查询是它最大的缺点。**

试想一种情况:

```
SELECT * FROM tb1 WHERE id < 500;
```

B+树是有序的，在这种范围查询中，优势非常大，直接遍历比 500 小的叶子节点就够了。而 Hash 索引是根据 hash 算法来定位的，难不成还要把 1 - 499 的数据，每个都进行一次 hash 计算来定位吗?这就是 Hash 最大的缺点了。

### 索引类型

#### 主键索引(Primary Key)

数据表的主键列使用的就是主键索引。

一张数据表有只能有一个主键，并且主键不能为 null，不能重复。

在 mysql 的 InnoDB 的表中，当没有显示的指定表的主键时，InnoDB 会自动先检查表中是否有唯一索引的字段，如果有，则选择该字段为默认的主键，否则 InnoDB 将会自动创建一个 6 Byte 的自增主键。

#### 二级索引(辅助索引)

二级索引又称为辅助索引，是因为**二级索引的叶子节点存储的数据是主键**。也就是说，通过**二级索引，可以定位主键的位置。**

唯一索引，普通索引，前缀索引等索引属于二级索引。

**PS:不懂的同学可以暂存疑，慢慢往下看，后面会有答案的，也可以自行搜索。**

1. **唯一索引(Unique Key)** ：唯一索引也是一种约束。**唯一索引的属性列不能出现重复的数据，但是允许数据为 NULL，一张表允许创建多个唯一索引。** 建立唯一索引的目的大部分时候都是为了该属性列的数据的唯一性，而不是为了查询效率。
2. **普通索引(Index)** ：**普通索引的唯一作用就是为了快速查询数据，一张表允许创建多个普通索引，并允许数据重复和 NULL。**
3. **前缀索引(Prefix)** ：前缀索引只适用于字符串类型的数据。前缀索引是对文本的前几个字符创建索引，相比普通索引建立的数据更小， 因为只取前几个字符。
4. **全文索引(Full Text)** ：全文索引主要是为了检索大文本数据中的关键字的信息，是目前搜索引擎数据库使用的一种技术。Mysql5.6 之前只有 MYISAM 引擎支持全文索引，5.6 之后 InnoDB 也支持了全文索引。

### 聚集索引与非聚集索引

#### 聚集索引

**聚集索引即索引结构和数据一起存放的索引。主键索引属于聚集索引。**

在 Mysql 中，InnoDB 引擎的表的 `.ibd`文件就包含了该表的索引和数据，对于 InnoDB 引擎表来说，该表的索引(B+树)的每个非叶子节点存储索引，叶子节点存储索引和索引对应的数据。

##### 聚集索引的优点

聚集索引的查询速度非常的快，因为整个 B+树本身就是一颗多叉平衡树，叶子节点也都是有序的，定位到索引的节点，就相当于定位到了数据。

##### 聚集索引的缺点

1. **依赖于有序的数据** ：因为 B+树是多路平衡树，如果索引的数据不是有序的，那么就需要在插入时排序，如果数据是整型还好，否则类似于字符串或 UUID 这种又长又难比较的数据，插入或查找的速度肯定比较慢。
2. **更新代价大** ： 如果对索引列的数据被修改时，那么对应的索引也将会被修改， 而且况聚集索引的叶子节点还存放着数据，修改代价肯定是较大的， 所以对于主键索引来说，主键一般都是不可被修改的。

#### 非聚集索引

**非聚集索引即索引结构和数据分开存放的索引。**

**二级索引属于非聚集索引。**

> MYISAM 引擎的表的.MYI 文件包含了表的索引， 该表的索引(B+树)的每个叶子非叶子节点存储索引， 叶子节点存储索引和索引对应数据的指针，指向.MYD 文件的数据。
>
> **非聚集索引的叶子节点并不一定存放数据的指针， 因为二级索引的叶子节点就存放的是主键，根据主键再回表查数据。**

##### 非聚集索引的优点

**更新代价比聚集索引要小** 。非聚集索引的更新代价就没有聚集索引那么大了，非聚集索引的叶子节点是不存放数据的

##### 非聚集索引的缺点

1. 跟聚集索引一样，非聚集索引也依赖于有序的数据
2. **可能会二次查询(回表)** :这应该是非聚集索引最大的缺点了。 当查到索引对应的指针或主键后，可能还需要根据指针或主键再到数据文件或表中查询。

这是 Mysql 的表的文件截图:

![Mysql表文件截图](https://snailclimb.gitee.io/javaguide/media/pictures/database/Mysql%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%88%AA%E5%9B%BE.png)

聚集索引和非聚集索引:

![B+树](https://snailclimb.gitee.io/javaguide/media/pictures/database/B+%E6%A0%91%E7%B4%A2%E5%BC%95.png)

### 索引优化相关

#### 1、索引相关的重要概念

基数：单个列唯一键（`distict_keys`）的数量叫做基数。
`SELECT COUNT(DISTINCT name),COUNT(DISTINCT gender) FROM user;`

![img](https://pic3.zhimg.com/80/v2-db4f716e689f04a1ee3a715c5bbfe34a_1440w.jpg)



user表的总行数是5，gender列的基数是2，**说明gender列里面有大量重复值**，name列的基数等于总行数，说明name列没有重复值，相当于主键。
返回数据的比例：user表中共有5条数据：

SELECT * FROM user;

![img](https://pic4.zhimg.com/80/v2-5e9dfb475ce969818218aae5e089720f_1440w.jpg)



查询满足性别为0（男）的记录数：

![img](https://pic1.zhimg.com/80/v2-c8667d3ae7f07a8dfe25a2fe8a7c3984_1440w.jpg)



那么返回记录的比例数是：

![img](https://pic4.zhimg.com/80/v2-fbe325f4c6326119745cf4ac780e8917_1440w.jpg)



同理，查询name为`'swj'`的记录数：

![img](https://pic4.zhimg.com/80/v2-a118ff47f406712754ff8dda040110fb_1440w.jpg)



返回记录的比例数是：

![img](https://pic4.zhimg.com/80/v2-a118ff47f406712754ff8dda040110fb_1440w.jpg)



现在问题来了，假设name、gender列都有索引，那么`SELECT * FROM user WHERE gender = 0; SELECT * FROM user WHERE name = 'swj';`都能命中索引吗？
user表的索引详情：

![img](https://pic1.zhimg.com/80/v2-2b6bd291abc99f58a9583b59a02ca650_1440w.png)



`SELECT * FROM user WHERE gender = 0;`没有命中索引，注意filtered的值就是上面我们计算的返回记录的比例数。

![img](https://pic4.zhimg.com/80/v2-ceaf5229a85fe08a03bb82ce3743ab67_1440w.png)



`SELECT * FROM user WHERE name = 'swj';`命中了索引index_name，因为走索引直接就能找到要查询的记录，所以filtered的值为100。

![img](https://pic4.zhimg.com/80/v2-7b3a43d3b90e488bfa861aedc1898e3f_1440w.png)



因此，返回表中30%内的数据会走索引，返回超过30%数据就使用全表扫描。当然这个结论太绝对了，也并不是绝对的30%，只是一个大概的范围。
回表：当对一个列创建索引之后，索引会包含该列的键值及键值对应行所在的`rowid`。通过索引中记录的`rowid`访问表中的数据就叫回表。回表次数太多会严重影响SQL性能，如果回表次数太多，就不应该走索引扫描，应该直接走全表扫描。
EXPLAIN命令结果中的Using Index意味着不会回表，通过索引就可以获得主要的数据。Using Where则意味着需要回表取数据。

#### 2. 索引优化实战

有些时候虽然数据库有索引，但是并不被优化器选择使用。我们可以通过SHOW STATUS LIKE 'Handler_read%';查看索引的使用情况：

![img](https://pic3.zhimg.com/80/v2-e638ac7ea1437cc731ab2c6c878760b2_1440w.jpg)



**Handler_read_key**：如果索引正在工作，Handler_read_key的值将很高。
**Handler_read_rnd_next**：数据文件中读取下一行的请求数，如果正在进行大量的表扫描，值将较高，则说明索引利用不理想。

#### 索引优化规则：

**1）如果MySQL估计使用索引比全表扫描还慢，则不会使用索引。**
返回数据的比例是重要的指标，比例越低越容易命中索引。记住这个范围值——**30%**，后面所讲的内容都是建立在返回数据的比例在**30%**以内的基础上。
**2）前导模糊查询不能命中索引。**
name列创建普通索引：

![img](https://pic4.zhimg.com/80/v2-3022332d4839dfd0c4a4a38d1b456713_1440w.png)



前导模糊查询不能命中索引：
EXPLAIN SELECT * FROM user WHERE name LIKE '%s%';

![img](https://pic1.zhimg.com/80/v2-c533d78ff310fda90f61d43dfc319260_1440w.png)



非前导模糊查询则可以使用索引，可优化为使用非前导模糊查询：
EXPLAIN SELECT * FROM user WHERE name LIKE 's%';

![img](https://pic4.zhimg.com/80/v2-e24d6abd1201a3e84ea6ee014bfbaa3b_1440w.png)



**3）数据类型出现隐式转换的时候不会命中索引，特别是当列类型是字符串，一定要将字符常量值用引号引起来。**
EXPLAIN SELECT * FROM user WHERE name=1;

![img](https://pic4.zhimg.com/80/v2-3d1eb9b46263283dc446812f6f143ff7_1440w.png)



EXPLAIN SELECT * FROM user WHERE name='1';

![img](https://pic2.zhimg.com/80/v2-53271b837814c4de3e64c0faf98e66a1_1440w.png)



**4）复合索引的情况下，查询条件不包含索引列最左边部分（不满足最左原则），不会命中符合索引。**
**name,age,status列创建复合索引：**

ALTER TABLE user ADD INDEX index_name (name,age,status);

![img](https://pic2.zhimg.com/80/v2-a631435e285567de6b53510e0765d565_1440w.png)



user表索引详情：
SHOW INDEX FROM user;

![img](https://pic2.zhimg.com/80/v2-903c1a677e70e82c719d2722a3b8b201_1440w.jpg)



根据最左原则，可以命中复合索引index_name：
EXPLAIN SELECT * FROM user WHERE name='swj' AND status=1;

![img](https://pic1.zhimg.com/80/v2-9e9a020b0e6de0dae7c8878da7b5ad6c_1440w.jpg)



注意，最左原则并不是说是查询条件的顺序：
EXPLAIN SELECT * FROM user WHERE status=1 AND name='swj';

![img](https://pic4.zhimg.com/80/v2-801bf32abe662adf98fd7e0465f31d93_1440w.jpg)



而是查询条件中是否包含索引最左列字段：
EXPLAIN SELECT * FROM user WHERE status=2 ;

![img](https://pic2.zhimg.com/80/v2-d266b262f1ef5a4d936c9bf943d308b9_1440w.jpg)



**5）union、in、or都能够命中索引，建议使用in。**
**union:**

EXPLAIN SELECT*FROM user WHERE status=1

UNION ALL

SELECT*FROM user WHERE status = 2;

![img](https://pic4.zhimg.com/80/v2-2c1b357a2dbf40f30e6532a7a0eaab6b_1440w.jpg)



in:
EXPLAIN SELECT * FROM user WHERE status IN (1,2);

![img](https://pic2.zhimg.com/80/v2-f35732fd1b2391d9c518f62b3f9d6005_1440w.jpg)



or:
EXPLAIN SELECT*FROM user WHERE status=1OR status=2;

![img](https://pic1.zhimg.com/80/v2-a46ec54803e69cbedf5749ef5ab28928_1440w.jpg)



查询的CPU消耗：or>in>union。
**6）用or分割开的条件，如果or前的条件中列有索引，而后面的列中没有索引，那么涉及到的索引都不会被用到。**
EXPLAIN SELECT * FROM payment WHERE customer_id = 203 OR amount = 3.96;

![img](https://pic3.zhimg.com/80/v2-32a2ed14c156a4b9cc15a660575a4b0e_1440w.jpg)



因为or后面的条件列中没有索引，那么后面的查询肯定要走全表扫描，在存在全表扫描的情况下，就没有必要多一次索引扫描增加IO访问。
**7）负向条件查询不能使用索引，可以优化为in查询。**
负向条件有：!=、<>、not in、not exists、not like等。
status列创建索引：

ALTER TABLE user ADD INDEX index_status (status);

![img](https://pic3.zhimg.com/80/v2-16f77f6d1a2fb6bea455bde299a9fb0a_1440w.png)



user表索引详情：
SHOW INDEX FROM user;

![img](https://pic3.zhimg.com/80/v2-bfd610d80b0b1b8918cf01c24a9a11fe_1440w.jpg)



负向条件不能命中缓存：
EXPLAIN SELECT * FROM user WHERE status !=1 AND status != 2;

![img](https://pic1.zhimg.com/80/v2-adec6a558dbd4e14883dabd1e459ef78_1440w.jpg)



可以优化为in查询，但是前提是区分度要高，返回数据的比例在30%以内：
EXPLAIN SELECT * FROM user WHERE status IN (0,3,4);

![img](https://pic4.zhimg.com/80/v2-e29fa30573ae029d070135c0538d277b_1440w.jpg)



**8）范围条件查询可以命中索引。范围条件有：<、<=、>、>=、between等。**
**status,age列分别创建索引：**

ALTER TABLE user ADD INDEX index_status (status);

![img](https://pic3.zhimg.com/80/v2-16f77f6d1a2fb6bea455bde299a9fb0a_1440w.png)



ALTER TABLE user ADD INDEX index_age (age);

![img](https://pic1.zhimg.com/80/v2-9af30cdb3268b4e3ab7559ca9686e900_1440w.png)



user表索引详情：
SHOW INDEX FROM user;

![img](https://pic2.zhimg.com/80/v2-566b088a90c3820dc83bc9dd5e441c05_1440w.jpg)



范围条件查询可以命中索引：
EXPLAIN SELECT * FROM user WHERE status>5;

![img](https://pic1.zhimg.com/80/v2-1cb3de6419a7029f51e29b64dd54f1c8_1440w.jpg)



范围列可以用到索引（联合索引必须是最左前缀），但是范围列后面的列无法用到索引，索引最多用于一个范围列，如果查询条件中有两个范围列则无法全用到索引：
EXPLAIN SELECT * FROM user WHERE status>5 AND age<24;

![img](https://pic3.zhimg.com/80/v2-94066a205562faec616624254398d6c2_1440w.jpg)



如果是范围查询和等值查询同时存在，优先匹配等值查询列的索引：
EXPLAIN SELECT * FROM user WHERE status>5 AND age=24;

![img](https://pic4.zhimg.com/80/v2-1dec05aa5802218d73f769d011d7fb8f_1440w.png)



**9）数据库执行计算不会命中索引。**
EXPLAIN SELECT * FROM user WHERE age>24;

![img](https://pic2.zhimg.com/80/v2-d71188f7ba07faf8f2f0f2f50b07a0cd_1440w.png)



EXPLAIN SELECT * FROM user WHERE age+1>24;

![img](https://pic4.zhimg.com/80/v2-c3cccb2d7d9d3092add21531674e4ad3_1440w.jpg)



计算逻辑应该尽量放到业务层处理，节省数据库的CPU的同时最大限度的命中索引。
**10）利用覆盖索引进行查询，避免回表。**
被查询的列，数据能从索引中取得，而不用通过行定位符row-locator再到row上获取，即“被查询列要被所建的索引覆盖”，这能够加速查询速度。
user表的索引详情：

![img](https://pic2.zhimg.com/80/v2-2c12bccbb0550f0542657516c64eb3b1_1440w.jpg)



因为status字段是索引列，所以直接从索引中就可以获取值，不必回表查询：
Using Index代表从索引中查询：

EXPLAIN SELECT status FROM user where status=1;

![img](https://pic2.zhimg.com/80/v2-c5a10f740ccc72711788d248915aecc5_1440w.png)



当查询其他列时，就需要回表查询，这也是为什么要避免SELECT*的原因之一：
EXPLAIN SELECT * FROM user where status=1;

![img](https://pic2.zhimg.com/80/v2-711c2fa8f55b47123d2304a5410bbb39_1440w.png)



**11）建立索引的列，不允许为null。**
单列索引不存null值，复合索引不存全为null的值，如果列允许为null，可能会得到“不符合预期”的结果集，所以，请使用not null约束以及默认值。
remark列建立索引：

ALTER TABLE user ADD INDEX index_remark (remark);

![img](https://pic2.zhimg.com/80/v2-edf2b9cde288a6d529665b528160c301_1440w.png)



IS NULL可以命中索引：
EXPLAIN SELECT * FROM user WHERE remark IS NULL;

![img](https://pic1.zhimg.com/80/v2-ba7f7ff9b4645c5dfdda3209c7bdf590_1440w.png)



IS NOT NULL不能命中索引：
EXPLAIN SELECT * FROM user WHERE remark IS NOT NULL;

![img](https://pic4.zhimg.com/80/v2-a94dba2acafd0cb5fe73464cbd2d691b_1440w.png)



虽然IS NULL可以命中索引，但是NULL本身就不是一种好的数据库设计，应该使用NOT NULL约束以及默认值。
a. 更新十分频繁的字段上不宜建立索引：因为更新操作会变更B+树，重建索引。这个过程是十分消耗数据库性能的。
b. 区分度不大的字段上不宜建立索引：类似于性别这种区分度不大的字段，建立索引的意义不大。因为不能有效过滤数据，性能和全表扫描相当。另外返回数据的比例在30%以外的情况下，优化器不会选择使用索引。
c. 业务上具有唯一特性的字段，即使是多个字段的组合，也必须建成唯一索引。虽然唯一索引会影响insert速度，但是对于查询的速度提升是非常明显的。另外，即使在应用层做了非常完善的校验控制，只要没有唯一索引，在并发的情况下，依然有脏数据产生。
d. 多表关联时，要保证关联字段上一定有索引。
e. 创建索引时避免以下错误观念：索引越多越好，认为一个查询就需要建一个索引；宁缺勿滥，认为索引会消耗空间、严重拖慢更新和新增速度；抵制唯一索引，认为业务的唯一性一律需要在应用层通过“先查后插”方式解决；过早优化，在不了解系统的情况下就开始优化。
**3. 小结**
对于自己编写的SQL查询语句，要尽量使用EXPLAIN命令分析一下，做一个对SQL性能有追求的程序员。衡量一个程序员是否靠谱，SQL能力是一个重要的指标。作为后端程序员，深以为然。

## 四、数据库并发控制——锁和MVCC

如果数据库中的所有事务都是串行执行的，那么它非常容易成为整个应用的性能瓶颈，虽然说没法水平扩展的节点在最后都会成为瓶颈，但是串行执行事务的数据库会加速这一过程；而并发（Concurrency）使一切事情的发生都有了可能，它能够解决一定的性能问题，但是它会带来更多诡异的错误。

引入了并发事务之后，如果不对事务的执行进行控制就会出现各种各样的问题，你可能没有享受到并发带来的性能提升就已经被各种奇怪的问题折磨的欲仙欲死了。

### 概述

如何控制并发是数据库领域中非常重要的问题之一，不过到今天为止事务并发的控制已经有了很多成熟的解决方案，而这些方案的原理就是这篇文章想要介绍的内容，文章中会介绍最为常见的三种并发控制机制：

![pessimistic-optimistic-multiversion-conccurency-control](https://img.draveness.me/2017-10-02-pessimistic-optimistic-multiversion-conccurency-control.png)

分别是悲观并发控制、乐观并发控制和多版本并发控制，其中悲观并发控制其实是最常见的并发控制机制，也就是锁；而乐观并发控制其实也有另一个名字：乐观锁，乐观锁其实并不是一种真实存在的锁，我们会在文章后面的部分中具体介绍；最后就是多版本并发控制（MVCC）了，与前两者对立的命名不同，MVCC 可以与前两者中的任意一种机制结合使用，以提高数据库的读性能。

既然这篇文章介绍了不同的并发控制机制，那么一定会涉及到不同事务的并发，我们会通过示意图的方式分析各种机制是如何工作的。

### 悲观并发控制

**控制不同的事务对同一份数据的获取**是保证数据库的一致性的最根本方法，如果我们能够让事务在同一时间对同一资源有着独占的能力，那么就可以保证操作同一资源的不同事务不会相互影响。

![pessimistic-conccurency-control](https://img.draveness.me/2017-10-02-pessimistic-conccurency-control.png)

最简单的、应用最广的方法就是使用锁来解决，**当事务需要对资源进行操作时需要先获得资源对应的锁**，保证其他事务不会访问该资源后，在对资源进行各种操作；在悲观并发控制中，数据库程序对于数据被修改持悲观的态度，在数据处理的过程中都会被锁定，以此来解决竞争的问题。

### 读写锁

为了最大化数据库事务的并发能力，数据库中的锁被设计为两种模式，分别是共享锁和互斥锁。当一个事务获得共享锁之后，它只可以进行读操作，所以共享锁也叫读锁；而当一个事务获得一行数据的互斥锁时，就可以对该行数据进行读和写操作，所以互斥锁也叫写锁。

![Shared-Exclusive-Lock](https://img.draveness.me/2017-10-02-Shared-Exclusive-Lock.png)

共享锁和互斥锁除了限制事务能够执行的读写操作之外，它们之间还有『共享』和『互斥』的关系，也就是多个事务可以同时获得某一行数据的共享锁，但是互斥锁与共享锁和其他的互斥锁并不兼容，我们可以很自然地理解这么设计的原因：多个事务同时写入同一数据难免会发生各种诡异的问题。

### 两阶段锁协议

两阶段锁协议（2PL）是一种能够保证事务可串行化的协议，它将事务的获取锁和释放锁划分成了增长（Growing）和缩减（Shrinking）两个不同的阶段。

![growing-to-shrinking](https://img.draveness.me/2017-10-02-growing-to-shrinking.png)

在增长阶段，一个事务可以获得锁但是不能释放锁；而在缩减阶段事务只可以释放锁，并不能获得新的锁，如果只看 2PL 的定义，那么到这里就已经介绍完了，但是它还有两个变种：

1. **Strict 2PL**：事务持有的**互斥**锁必须在提交后再释放；
2. **Rigorous 2PL**：事务持有的**所有**锁必须在提交后释放；

![two-phase-locking](https://img.draveness.me/2017-10-02-two-phase-locking.png)

虽然锁的使用能够为我们解决不同事务之间由于并发执行造成的问题，但是两阶段锁的使用却引入了另一个严重的问题，死锁；不同的事务等待对方已经锁定的资源就会造成死锁，我们在这里举一个简单的例子：

![deadlock](https://img.draveness.me/2017-10-02-deadlock.png)

两个事务在刚开始时分别获取了 draven 和 beacon 资源上面的锁，然后再请求对方已经获得的锁时就会发生死锁，双方都没有办法等到锁的释放，如果没有死锁的处理机制就会无限等待下去，两个事务都没有办法完成。

### 死锁的处理

死锁在多线程编程中是经常遇到的事情，一旦涉及多个线程对资源进行争夺就需要考虑当前的几个线程或者事务是否会造成死锁；解决死锁大体来看有两种办法，一种是从源头杜绝死锁的产生和出现，另一种是允许系统进入死锁的状态，但是在系统出现死锁时能够及时发现并且进行恢复。

![deadlock-handling](https://img.draveness.me/2017-10-02-deadlock-handling.png)

#### 预防死锁

有两种方式可以帮助我们预防死锁的出现，一种是保证事务之间的等待不会出现环，也就是事务之间的等待图应该是一张**有向无环图**，没有循环等待的情况或者保证一个事务中想要获得的所有资源都在事务开始时以原子的方式被锁定，所有的资源要么被锁定要么都不被锁定。

但是这种方式有两个问题，在事务一开始时很难判断哪些资源是需要锁定的，同时因为一些很晚才会用到的数据被提前锁定，数据的利用率与事务的并发率也非常的低。一种解决的办法就是按照一定的顺序为所有的数据行加锁，同时与 2PL 协议结合，在加锁阶段保证所有的数据行都是从小到大依次进行加锁的，不过这种方式依然需要事务提前知道将要加锁的数据集。

另一种预防死锁的方法就是使用抢占加事务回滚的方式预防死锁，当事务开始执行时会先获得一个时间戳，数据库程序会根据事务的时间戳决定事务应该等待还是回滚，在这时也有两种机制供我们选择，一种是 wait-die 机制：

![deadlock-prevention-wait-die](https://img.draveness.me/2017-10-02-deadlock-prevention-wait-die.png)

当执行事务的时间戳小于另一事务时，即事务 A 先于 B 开始，那么它就会等待另一个事务释放对应资源的锁，否则就会保持当前的时间戳并回滚。

另一种机制叫做 wound-wait，这是一种抢占的解决方案，它和 wait-die 机制的结果完全相反，当前事务如果先于另一事务执行并请求了另一事务的资源，那么另一事务会立刻回滚，将资源让给先执行的事务，否则就会等待其他事务释放资源：

![deadlock-prevention-wound-wait](https://img.draveness.me/2017-10-02-deadlock-prevention-wound-wait.png)

两种方法都会造成不必要的事务回滚，由此会带来一定的性能损失，更简单的解决死锁的方式就是使用超时时间，但是超时时间的设定是需要仔细考虑的，否则会造成耗时较长的事务无法正常执行，或者无法及时发现需要解决的死锁，所以它的使用还是有一定的局限性。

### 死锁检测和恢复

如果数据库程序无法通过协议从原理上保证死锁不会发生，那么就需要在**死锁发生时及时检测到并从死锁状态恢复到正常状态保证数据库程序可以正常工作**。在使用检测和恢复的方式解决死锁时，数据库程序需要维护数据和事务之间的引用信息，同时也需要提供一个用于判断当前数据库是否进入死锁状态的算法，最后需要在死锁发生时提供合适的策略及时恢复。

在上一节中我们其实提到死锁的检测可以通过一个有向的等待图来进行判断，如果一个事务依赖于另一个事务正在处理的数据，那么当前事务就会等待另一个事务的结束，这也就是整个等待图中的一条边：

![deadlock-wait-for-graph](https://img.draveness.me/2017-10-02-deadlock-wait-for-graph.png)

如上图所示，如果在这个有向图中出现了环，就说明当前数据库进入了死锁的状态 `TransB -> TransE -> TransF -> TransD -> TransB`，在这时就需要死锁恢复机制接入了。

如何从死锁中恢复其实非常简单，最常见的解决办法就是选择整个环中一个事务进行回滚，以打破整个等待图中的环，在整个恢复的过程中有三个事情需要考虑：

![deadlock-recovery](https://img.draveness.me/2017-10-02-deadlock-recovery.png)

每次出现死锁时其实都会有多个事务被波及，而选择其中哪一个任务进行回滚是必须要做的事情，在选择牺牲品（Victim）时的黄金原则就是**最小化代价**，所以我们需要综合考虑事务已经计算的时间、使用的数据行以及涉及的事务等因素；当我们选择了牺牲品之后就可以开始回滚了，回滚其实有两种选择一种是全部回滚，另一种是部分回滚，部分回滚会回滚到事务之前的一个检查点上，如果没有检查点那自然没有办法进行部分回滚。

> 在死锁恢复的过程中，其实还可能出现某些任务在多次死锁时都被选择成为牺牲品，一直都不会成功执行，造成饥饿（Starvation），我们需要保证事务会在有穷的时间内执行，所以要在选择牺牲品时将时间戳加入考虑的范围。

### 锁的粒度

到目前为止我们都没有对不同粒度的锁进行讨论，一直以来我们都讨论的都是数据行锁，但是在有些时候我们希望将多个节点看做一个数据单元，使用锁直接将这个数据单元、表甚至数据库锁定起来。这个目标的实现需要我们在数据库中定义不同粒度的锁：

![granularity-hierarchy](https://img.draveness.me/2017-10-02-granularity-hierarchy.png)

当我们拥有了不同粒度的锁之后，如果某个事务想要锁定整个数据库或者整张表时只需要简单的锁住对应的节点就会在当前节点加上显示（explicit）锁，在所有的子节点上加隐式（implicit）锁；虽然这种不同粒度的锁能够解决父节点被加锁时，子节点不能被加锁的问题，但是我们没有办法在子节点被加锁时，立刻确定父节点不能被加锁。

在这时我们就需要引入*意向锁*来解决这个问题了，当需要给子节点加锁时，先给所有的父节点加对应的意向锁，意向锁之间是完全不会互斥的，只是用来帮助父节点快速判断是否可以对该节点进行加锁：

![lock-type-compatibility-matrix](https://img.draveness.me/2017-10-02-lock-type-compatibility-matrix.png)

这里是一张引入了两种意向锁，*意向共享锁*和*意向互斥锁*之后所有的锁之间的兼容关系；到这里，我们通过不同粒度的锁和意向锁加快了数据库的吞吐量。

#### 乐观并发控制

除了悲观并发控制机制 - 锁之外，我们其实还有其他的并发控制机制，*乐观并发控制*（Optimistic Concurrency Control）。乐观并发控制也叫乐观锁，但是它并不是真正的锁，很多人都会误以为乐观锁是一种真正的锁，然而它只是一种并发控制的思想。

![pessimistic-and-optimisti](https://img.draveness.me/2017-10-02-pessimistic-and-optimistic.png)

在这一节中，我们将会先介绍*基于时间戳的并发控制机制*，然后在这个协议的基础上进行扩展，实现乐观的并发控制机制。

### 基于时间戳的协议

锁协议按照不同事务对同一数据项请求的时间依次执行，因为后面执行的事务想要获取的数据已将被前面的事务加锁，只能等待锁的释放，所以基于锁的协议执行事务的顺序与获得锁的顺序有关。在这里想要介绍的基于时间戳的协议能够在事务执行之前先决定事务的执行顺序。

**每一个事务都会具有一个全局唯一的时间戳**，它即可以使用系统的时钟时间，也可以使用计数器，只要能够保证所有的时间戳都是唯一并且是随时间递增的就可以。

![timestamp-ordering-protocol](https://img.draveness.me/2017-10-02-timestamp-ordering-protocol.png)

基于时间戳的协议能够保证事务并行执行的顺序与事务按照时间戳串行执行的效果完全相同；每一个数据项都有两个时间戳，读时间戳和写时间戳，分别代表了当前成功执行对应操作的事务的时间戳。

该协议能够保证所有冲突的读写操作都能按照时间戳的大小串行执行，在执行对应的操作时不需要关注其他的事务只需要关心数据项对应时间戳的值就可以了：

![timestamp-ordering-protocol-process](https://img.draveness.me/2017-10-02-timestamp-ordering-protocol-process.png)

无论是读操作还是写操作都会从左到右依次比较读写时间戳的值，如果小于当前值就会直接被拒绝然后回滚，数据库系统会给回滚的事务添加一个新的时间戳并重新执行这个事务。

### 基于验证的协议

*乐观并发控制*其实本质上就是基于验证的协议，因为在多数的应用中只读的事务占了绝大多数，事务之间因为写操作造成冲突的可能非常小，也就是说大多数的事务在不需要并发控制机制也能运行的非常好，也可以保证数据库的一致性；而并发控制机制其实向整个数据库系统添加了很多的开销，我们其实可以通过别的策略降低这部分开销。

而验证协议就是我们找到的解决办法，它根据事务的只读或者更新将所有事务的执行分为两到三个阶段：

![validation-based-protoco](https://img.draveness.me/2017-10-02-validation-based-protocol.png)

在读阶段，数据库会执行事务中的**全部读操作和写操作**，并将所有写后的值存入临时变量中，并不会真正更新数据库中的内容；在这时候会进入下一个阶段，数据库程序会检查当前的改动是否合法，也就是是否有其他事务在 RAED PHASE 期间更新了数据，如果通过测试那么直接就进入 WRITE PHASE 将所有存在临时变量中的改动全部写入数据库，没有通过测试的事务会直接被终止。

为了保证乐观并发控制能够正常运行，我们需要知道一个事务不同阶段的发生时间，包括事务开始时间、验证阶段的开始时间以及写阶段的结束时间；通过这三个时间戳，我们可以保证任意冲突的事务不会同时写入数据库，一旦由一个事务完成了验证阶段就会立即写入，其他读取了相同数据的事务就会回滚重新执行。

作为乐观的并发控制机制，它会假定所有的事务在最终都会通过验证阶段并且执行成功，而锁机制和基于时间戳排序的协议是悲观的，因为它们会在发生冲突时强制事务进行等待或者回滚，哪怕有不需要锁也能够保证事务之间不会冲突的可能。

#### 多版本并发控制

到目前为止我们介绍的并发控制机制其实都是通过延迟或者终止相应的事务来解决事务之间的竞争条件（Race condition）来保证事务的可串行化；虽然前面的两种并发控制机制确实能够从根本上解决并发事务的可串行化的问题，但是在实际环境中数据库的事务大都是只读的，读请求是写请求的很多倍，如果写请求和读请求之前没有并发控制机制，那么最坏的情况也是读请求读到了已经写入的数据，这对很多应用完全是可以接受的。

![multiversion-scheme](https://img.draveness.me/2017-10-02-multiversion-scheme.png)

在这种大前提下，数据库系统引入了另一种并发控制机制 - *多版本并发控制*（Multi-version Concurrency Control），每一个写操作都会创建一个新版本的数据，读操作会从有限多个版本的数据中挑选一个最合适的结果直接返回；在这时，读写操作之间的冲突就不再需要被关注，而管理和快速挑选数据的版本就成了 MVCC 需要解决的主要问题。

MVCC 并不是一个与乐观和悲观并发控制对立的东西，它能够与两者很好的结合以增加事务的并发量，在目前最流行的 SQL 数据库 MySQL 和 PostgreSQL 中都对 MVCC 进行了实现；但是由于它们分别实现了悲观锁和乐观锁，所以 MVCC 实现的方式也不同。

### MySQL 与 MVCC

MySQL 中实现的多版本两阶段锁协议（Multiversion 2PL）将 MVCC 和 2PL 的优点结合了起来，每一个版本的数据行都具有一个唯一的时间戳，当有读事务请求时，数据库程序会直接从多个版本的数据项中具有最大时间戳的返回。

![multiversion-2pl-read](https://img.draveness.me/2017-10-02-multiversion-2pl-read.png)

更新操作就稍微有些复杂了，事务会先读取最新版本的数据计算出数据更新后的结果，然后创建一个新版本的数据，新数据的时间戳是目前数据行的最大版本 `＋1`：

![multiversion-2pl-write](https://img.draveness.me/2017-10-02-multiversion-2pl-write.png)

数据版本的删除也是根据时间戳来选择的，MySQL 会将版本最低的数据定时从数据库中清除以保证不会出现大量的遗留内容。

### PostgreSQL 与 MVCC

与 MySQL 中使用悲观并发控制不同，PostgreSQL 中都是使用乐观并发控制的，这也就导致了 MVCC 在于乐观锁结合时的实现上有一些不同，最终实现的叫做多版本时间戳排序协议（Multiversion Timestamp Ordering），在这个协议中，所有的事务在执行之前都会被分配一个唯一的时间戳，每一个数据项都有读写两个时间戳：

![dataitem-with-timestamps](https://img.draveness.me/2017-10-02-dataitem-with-timestamps.png)

当 PostgreSQL 的事务发出了一个读请求，数据库直接将最新版本的数据返回，不会被任何操作阻塞，而写操作在执行时，事务的时间戳一定要大或者等于数据行的读时间戳，否则就会被回滚。

这种 MVCC 的实现保证了读事务永远都不会失败并且不需要等待锁的释放，对于读请求远远多于写请求的应用程序，乐观锁加 MVCC 对数据库的性能有着非常大的提升；虽然这种协议能够针对一些实际情况做出一些明显的性能提升，但是也会导致两个问题，一个是每一次读操作都会更新读时间戳造成两次的磁盘写入，第二是事务之间的冲突是通过回滚解决的，所以如果冲突的可能性非常高或者回滚代价巨大，数据库的读写性能还不如使用传统的锁等待方式。

## 总结

数据库的并发控制机制到今天已经有了非常成熟、完善的解决方案，我们并不需要自己去设计一套新的协议来处理不同事务之间的冲突问题，从数据库的并发控制机制中学习到的相关知识，无论是锁还是乐观并发控制在其他的领域或者应用中都被广泛使用，所以了解、熟悉不同的并发控制机制的原理是很有必要的。

